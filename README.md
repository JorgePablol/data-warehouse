# Data warehouse with AWS

This project processes files storaged in s3 into a staging table and then loads the data into 5 different tables on redshift, all of this was done for the streaming music startup sparkify, that needs to move their information into the cloud.

## Project files

* create_table.py: Creates the fact and dimension tables.
* etl.py: Loads the dataset from s3 and then takes it to redshift.
* sql_queries.py: The sql statements that create, drop and copy the data.

## Datasets
#### Song dataset
It's a subset of real data from [Million Song Dataset](https://labrosa.ee.columbia.edu/millionsong/). Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID.

Sample Data:
```
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
```
#### Log Dataset
The second dataset consists of log files in JSON format generated by this  [event simulator](https://github.com/Interana/eventsim)  based on the songs in the dataset above. These simulate app activity logs from an imaginary music streaming app based on configuration settings.

The log files in the dataset are partitioned by year and month. 

Sample Data: 

    {"artist":null,"auth":"Logged In","firstName":"Walter","gender":"M","itemInSession":0,"lastName":"Frye","length":null,"level":"free","location":"San Francisco-Oakland-Hayward, CA","method":"GET","page":"Home","registration":1540919166796.0,"sessionId":38,"song":null,"status":200,"ts":1541105830796,"userAgent":"\"Mozilla\/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit\/537.36 (KHTML, like Gecko) Chrome\/36.0.1985.143 Safari\/537.36\"","userId":"39"}


## Database tables
#### Fact Table

* **songplays** - records in event data associated with song plays i.e. records with page NextSong
        songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent

#### Dimension Tables

* **users** - users in the app
        user_id, first_name, last_name, gender, level
* **songs** - songs in music database
        song_id, title, artist_id, year, duration
* **artists** - artists in music database
        artist_id, name, location, lattitude, longitude
* **time** - timestamps of records in songplays broken down into specific units
        start_time, hour, day, week, month, year, weekday


## Run the scripts
1. Create a redshift cluster in aws, also vpc and upload files on s3.
2. Write a data warehouse configuration files that allows you to query redshift with code.
3. Open a terminal.
4. Write **python create_tables.py**: This one goes first so you have tables in your db.
5. Write **python etl.py**: To execute the etl, you are done.
